# Module 4: Vision-Language-Action (VLA)

## Overview
The convergence of Large Language Models (LLMs) and Robotics. This module bridges the gap between natural language and physical action.

## Learning Objectives
- **Voice-to-Action**: Use OpenAI Whisper for voice commands.
- **Cognitive Planning**: Use LLMs to translate "Clean the room" into ROS 2 actions.
- **Capstone Project**: The Autonomous Humanoid.

## Key Concepts
### 1. VLA Models
Models that take Vision and Language as input and output Robot Actions.

### 2. OpenAI Whisper
A robust speech recognition model to give the robot a voice interface.

### 3. Cognitive Planning
The ability of an AI to break down a high-level goal into executable steps (e.g., Navigate -> Detect -> Grasp).

## Weekly Breakdown
- **Week 13**: Conversational Robotics, Multi-modal interaction.
